{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXMX8yuAyl55"
      },
      "source": [
        "# Welcome to CS 5242 **Assignment 6**\n",
        "\n",
        "ASSIGNMENT DEADLINE â° : ** 18 April 2024**\n",
        "\n",
        "In this assignment, we learn how to adopt a parameter-efficient fine-tuning (PEFT) method to fine-tune a large model. The parameter-efficient fine-tuning mehtod is a very popular technique for large model training.\n",
        "\n",
        "we have three parts:\n",
        "1. Load pre-trained parameters correctly. 2 points\n",
        "2. Implement an adapter module and insert it into your model.  6 points\n",
        "3. Fine-tuning the model for 1 epoch. 2 points.\n",
        "\n",
        "Colab is a hosted Jupyter notebook service that requires no setup to use, while providing access free of charge to computing resources including GPUs. In this semester, we will use Colab to run our experiments.\n",
        "1. Login Google Colab https://colab.research.google.com/\n",
        "2. In this assignment, We **need GPU** to training the CNN model. You may need to **choose GPU in Runtime -> Change runtime type -> Hardware accerator**\n",
        "\n",
        "\n",
        "\n",
        "### **Grades Policy**\n",
        "\n",
        "We have 10 points for this homework. 15% off per day late, 0 scores if you submit it 7 days after the deadline.\n",
        "\n",
        "### **Cautions**\n",
        "\n",
        "**DO NOT** copy the code from the internet, e.g. GitHub.\n",
        "---\n",
        "\n",
        "**DO NOT** use any LLMs to write the code, e.g. ChatGPT.\n",
        "---\n",
        "\n",
        "### **Contact**\n",
        "\n",
        "Please feel free to contact us if you have any question about this homework or need any further information.\n",
        "\n",
        "Slack: Wangbo Zhao\n",
        "\n",
        "\n",
        "> If you have not join the slack group, you can click [here](https://join.slack.com/t/cs5242-2024spring/shared_invite/zt-2cw3jgqab-wFhoaIVa4RIX4fCZ_k~vjQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLeZHcOVBp4U"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Start by running the cell below to set up all required software."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIgu_q2HBg-E",
        "outputId": "cf7527e0-01b2-4d4f-d83b-58386ce64524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: timm==0.9.16 in /usr/local/lib/python3.10/dist-packages (0.9.16)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm==0.9.16) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.9.16) (0.17.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.16) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.16) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.16) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm==0.9.16) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm==0.9.16) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm==0.9.16) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm==0.9.16) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm==0.9.16) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm==0.9.16) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm==0.9.16) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm==0.9.16) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm==0.9.16) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm==0.9.16) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm==0.9.16) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm==0.9.16) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->timm==0.9.16) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm==0.9.16) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->timm==0.9.16) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm==0.9.16) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->timm==0.9.16) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm==0.9.16) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->timm==0.9.16) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm==0.9.16) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm==0.9.16) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm==0.9.16) (12.4.127)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.9.16) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.9.16) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm==0.9.16) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm==0.9.16) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm==0.9.16) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm==0.9.16) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm==0.9.16) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm==0.9.16) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy matplotlib\n",
        "!pip install torch torchvision\n",
        "!pip install timm==0.9.16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtXcchT5H2PH"
      },
      "source": [
        "Import the neccesary library and fix seed for Python, NumPy and PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2Yodsn4H6CB",
        "outputId": "69a7292c-9ad0-4f0c-c15d-12f44d3dd21d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a09d565b4b0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTpFBLKSkKI0"
      },
      "source": [
        "Now let's setup the GPU environment. The colab provides a free GPU to use. Do as follows:\n",
        "\n",
        "- Runtime -> Change Runtime Type -> select `GPU` in Hardware accelerator\n",
        "- Click `connect` on the top-right\n",
        "\n",
        "After connecting to one GPU, you can check its status using `nvidia-smi` command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES8KOxziiYky",
        "outputId": "32e58da3-06b2-4d35-935f-8faa241a068a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 16 10:54:27 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P0              28W /  70W |   8085MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yrZD7DDExF4"
      },
      "source": [
        "Everything is ready, you can move on and ***Good Luck !*** ðŸ˜ƒ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm1f362vdRgF"
      },
      "source": [
        "## Load parameters from pre-trained model.\n",
        "\n",
        "In this section, you need to load a checkpoint of the vision transformer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV3DXc2jgeg7"
      },
      "source": [
        "\n",
        "Implement the code for ViT-Base. Do not worry, I have done it for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3UxGJxTegq9O"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from collections import OrderedDict\n",
        "from typing import Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from typing import Any, Callable, Dict, Optional, Sequence, Set, Tuple, Type, Union, List\n",
        "from timm.models.layers import DropPath, trunc_normal_\n",
        "from torch.jit import Final\n",
        "from timm.layers import PatchEmbed, Mlp, DropPath, PatchDropout, trunc_normal_, use_fused_attn\n",
        "\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    fused_attn: Final[bool]\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            dim,\n",
        "            num_heads=8,\n",
        "            qkv_bias=False,\n",
        "            qk_norm=False,\n",
        "            attn_drop=0.,\n",
        "            proj_drop=0.,\n",
        "            norm_layer=nn.LayerNorm,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.fused_attn = use_fused_attn()\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
        "        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv.unbind(0)\n",
        "        q, k = self.q_norm(q), self.k_norm(k)\n",
        "\n",
        "        if self.fused_attn:\n",
        "            x = F.scaled_dot_product_attention(\n",
        "                q, k, v,\n",
        "                dropout_p=self.attn_drop.p,\n",
        "            )\n",
        "        else:\n",
        "            q = q * self.scale\n",
        "            attn = q @ k.transpose(-2, -1)\n",
        "            attn = attn.softmax(dim=-1)\n",
        "            attn = self.attn_drop(attn)\n",
        "            x = attn @ v\n",
        "\n",
        "        x = x.transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LayerScale(nn.Module):\n",
        "    def __init__(self, dim, init_values=1e-5, inplace=False):\n",
        "        super().__init__()\n",
        "        self.inplace = inplace\n",
        "        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            dim,\n",
        "            num_heads,\n",
        "            mlp_ratio=4.,\n",
        "            qkv_bias=False,\n",
        "            qk_norm=False,\n",
        "            proj_drop=0.,\n",
        "            attn_drop=0.,\n",
        "            init_values=None,\n",
        "            drop_path=0.,\n",
        "            act_layer=nn.GELU,\n",
        "            norm_layer=nn.LayerNorm,\n",
        "            mlp_layer=Mlp\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim,\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            qk_norm=qk_norm,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=proj_drop,\n",
        "            norm_layer=norm_layer,\n",
        "        )\n",
        "        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
        "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        self.mlp = mlp_layer(\n",
        "            in_features=dim,\n",
        "            hidden_features=int(dim * mlp_ratio),\n",
        "            act_layer=act_layer,\n",
        "            drop=proj_drop,\n",
        "        )\n",
        "        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
        "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n",
        "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer\n",
        "\n",
        "    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`\n",
        "        - https://arxiv.org/abs/2010.11929\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            img_size: Union[int, Tuple[int, int]] = 224,\n",
        "            patch_size: Union[int, Tuple[int, int]] = 16,\n",
        "            in_chans: int = 3,\n",
        "            num_classes: int = 1000,\n",
        "            global_pool: str = 'token',\n",
        "            embed_dim: int = 768,\n",
        "            depth: int = 12,\n",
        "            num_heads: int = 12,\n",
        "            mlp_ratio: float = 4.,\n",
        "            qkv_bias: bool = True,\n",
        "            qk_norm: bool = False,\n",
        "            init_values: Optional[float] = None,\n",
        "            class_token: bool = True,\n",
        "            no_embed_class: bool = False,\n",
        "            pre_norm: bool = False,\n",
        "            fc_norm: Optional[bool] = None,\n",
        "            drop_rate: float = 0.,\n",
        "            pos_drop_rate: float = 0.,\n",
        "            patch_drop_rate: float = 0.,\n",
        "            proj_drop_rate: float = 0.,\n",
        "            attn_drop_rate: float = 0.,\n",
        "            drop_path_rate: float = 0.,\n",
        "            weight_init: str = '',\n",
        "            embed_layer: Callable = PatchEmbed,\n",
        "            norm_layer: Optional[Callable] = None,\n",
        "            act_layer: Optional[Callable] = None,\n",
        "            block_fn: Callable = Block,\n",
        "            mlp_layer: Callable = Mlp\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_size: Input image size.\n",
        "            patch_size: Patch size.\n",
        "            in_chans: Number of image input channels.\n",
        "            num_classes: Mumber of classes for classification head.\n",
        "            global_pool: Type of global pooling for final sequence (default: 'token').\n",
        "            embed_dim: Transformer embedding dimension.\n",
        "            depth: Depth of transformer.\n",
        "            num_heads: Number of attention heads.\n",
        "            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n",
        "            qkv_bias: Enable bias for qkv projections if True.\n",
        "            init_values: Layer-scale init values (layer-scale enabled if not None).\n",
        "            class_token: Use class token.\n",
        "            fc_norm: Pre head norm after pool (instead of before), if None, enabled when global_pool == 'avg'.\n",
        "            drop_rate: Head dropout rate.\n",
        "            pos_drop_rate: Position embedding dropout rate.\n",
        "            attn_drop_rate: Attention dropout rate.\n",
        "            drop_path_rate: Stochastic depth rate.\n",
        "            weight_init: Weight initialization scheme.\n",
        "            embed_layer: Patch embedding layer.\n",
        "            norm_layer: Normalization layer.\n",
        "            act_layer: MLP activation layer.\n",
        "            block_fn: Transformer block layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert global_pool in ('', 'avg', 'token')\n",
        "        assert class_token or global_pool != 'token'\n",
        "        use_fc_norm = global_pool == 'avg' if fc_norm is None else fc_norm\n",
        "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
        "        act_layer = act_layer or nn.GELU\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.global_pool = global_pool\n",
        "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "        self.num_prefix_tokens = 1 if class_token else 0\n",
        "        self.no_embed_class = no_embed_class\n",
        "        self.grad_checkpointing = False\n",
        "\n",
        "        self.patch_embed = embed_layer(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            in_chans=in_chans,\n",
        "            embed_dim=embed_dim,\n",
        "            bias=not pre_norm,  # disable bias if pre-norm is used (e.g. CLIP)\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if class_token else None\n",
        "        embed_len = num_patches if no_embed_class else num_patches + self.num_prefix_tokens\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\n",
        "        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n",
        "        if patch_drop_rate > 0:\n",
        "            self.patch_drop = PatchDropout(\n",
        "                patch_drop_rate,\n",
        "                num_prefix_tokens=self.num_prefix_tokens,\n",
        "            )\n",
        "        else:\n",
        "            self.patch_drop = nn.Identity()\n",
        "        self.norm_pre = norm_layer(embed_dim) if pre_norm else nn.Identity()\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            block_fn(\n",
        "                dim=embed_dim,\n",
        "                num_heads=num_heads,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                qk_norm=qk_norm,\n",
        "                init_values=init_values,\n",
        "                proj_drop=proj_drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[i],\n",
        "                norm_layer=norm_layer,\n",
        "                act_layer=act_layer,\n",
        "                mlp_layer=mlp_layer\n",
        "            )\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim) if not use_fc_norm else nn.Identity()\n",
        "\n",
        "        # Classifier Head\n",
        "        self.fc_norm = norm_layer(embed_dim) if use_fc_norm else nn.Identity()\n",
        "        self.head_drop = nn.Dropout(drop_rate)\n",
        "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        if self.cls_token is not None:\n",
        "            nn.init.normal_(self.cls_token, std=1e-6)\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif hasattr(m, '_init_weights'):\n",
        "            m._init_weights()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed', 'cls_token', 'dist_token'}\n",
        "\n",
        "\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        if self.cls_token is not None:\n",
        "            x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        x = self.patch_drop(x)\n",
        "        x = self.norm_pre(x)\n",
        "\n",
        "\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def forward_head(self, x, pre_logits: bool = False):\n",
        "        if self.global_pool:\n",
        "            x = x[:, self.num_prefix_tokens:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]\n",
        "        x = self.fc_norm(x)\n",
        "        x = self.head_drop(x)\n",
        "        return x if pre_logits else self.head(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.forward_head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def convert_list_to_tensor(list_convert):\n",
        "    if len(list_convert):\n",
        "        result = torch.stack(list_convert, dim=1)\n",
        "    else :\n",
        "        result = None\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def vit_base_patch16_224_in21k(**kwargs):\n",
        "    \"\"\" ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
        "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
        "    NOTE: this model has valid 21k classifier head and no representation (pre-logits) layer\n",
        "    \"\"\"\n",
        "    model_kwargs = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True, **kwargs)\n",
        "    model = VisionTransformer(**model_kwargs)\n",
        "    return model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxnlbBnFw9wB"
      },
      "source": [
        "\n",
        "Download pre-trained model from \"https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_patch16_224_in21k-e5005f0a.pth\".\n",
        "Then load the parameters correctly."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_patch16_224_in21k-e5005f0a.pth /content/jx_vit_base_patch16_224_in21k-e5005f0a.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTyL_BrixYNK",
        "outputId": "a1804777-b03b-469c-b334-d9c067d18d8e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-16 10:54:27--  https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_patch16_224_in21k-e5005f0a.pth\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/168799526/9d25d380-4908-11eb-87fa-6afa7080e167?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240416%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240416T105428Z&X-Amz-Expires=300&X-Amz-Signature=46e6eebae6e3a0256ab49207305dd0356dcbf1c0a6d70c0a77ad6f98e9a582a4&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=168799526&response-content-disposition=attachment%3B%20filename%3Djx_vit_base_patch16_224_in21k-e5005f0a.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-04-16 10:54:28--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/168799526/9d25d380-4908-11eb-87fa-6afa7080e167?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240416%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240416T105428Z&X-Amz-Expires=300&X-Amz-Signature=46e6eebae6e3a0256ab49207305dd0356dcbf1c0a6d70c0a77ad6f98e9a582a4&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=168799526&response-content-disposition=attachment%3B%20filename%3Djx_vit_base_patch16_224_in21k-e5005f0a.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 412768565 (394M) [application/octet-stream]\n",
            "Saving to: â€˜jx_vit_base_patch16_224_in21k-e5005f0a.pth.2â€™\n",
            "\n",
            "jx_vit_base_patch16 100%[===================>] 393.65M  76.9MB/s    in 3.9s    \n",
            "\n",
            "2024-04-16 10:54:32 (102 MB/s) - â€˜jx_vit_base_patch16_224_in21k-e5005f0a.pth.2â€™ saved [412768565/412768565]\n",
            "\n",
            "/content/jx_vit_base_patch16_224_in21k-e5005f0a.pth: Scheme missing.\n",
            "FINISHED --2024-04-16 10:54:32--\n",
            "Total wall clock time: 4.5s\n",
            "Downloaded: 1 files, 394M in 3.9s (102 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "PBzDAo2rgwmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e69e1234-f1be-4f49-e84b-138586d125df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['pre_logits.fc.bias', 'pre_logits.fc.weight'])\n"
          ]
        }
      ],
      "source": [
        "model = vit_base_patch16_224_in21k(num_classes=100)\n",
        "checkpoint_model = torch.load(\"/content/jx_vit_base_patch16_224_in21k-e5005f0a.pth\")\n",
        "########## write code to load the checkpoint_model without error ##########\n",
        "new_model = dict()\n",
        "for key,value in checkpoint_model.items():\n",
        "  if 'head' not in key:\n",
        "    new_model[key] = value\n",
        "checkpoint_model = new_model\n",
        "########## write code to load the checkpoint_model without error ##########\n",
        "msg = model.load_state_dict(checkpoint_model, strict=False)\n",
        "print(msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUPU8RQf0Z-i"
      },
      "source": [
        "# Implement a adapter module.\n",
        "[AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition](https://proceedings.neurips.cc/paper_files/paper/2022/file/69e2f49ab0837b71b0e0cb7c555990f8-Paper-Conference.pdf)\n",
        "\n",
        "\n",
        "![Alt text](image.png)!\n",
        "The scale s is set to 0.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "xv5S88SJ0Z-j"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self,\n",
        "                 n_embd=768,\n",
        "                 down_size=8,\n",
        "                 dropout=0.0,\n",
        "                 adapter_scalar=\"0.1\"):\n",
        "        super().__init__()\n",
        "    ### implement the adapter module ####\n",
        "        self.down = nn.Linear(n_embd, down_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.up = nn.Linear(down_size, n_embd)\n",
        "        self.adapter_scaler = float(adapter_scalar)\n",
        "    ### implement the adapter module ####\n",
        "\n",
        "    def forward(self, x):\n",
        "    ### implement the adapter module ####\n",
        "        output = self.down(x)\n",
        "        output = self.relu(output)\n",
        "        output = self.up(output)\n",
        "        output = output * self.adapter_scaler\n",
        "        #print(1)\n",
        "\n",
        "    ### implement the adapter module ####\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL2O8SmY0Z-j"
      },
      "source": [
        "Insert the adapter module to your vision transformer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Bhb4lUkF0Z-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2732e674-7595-44b1-c819-962e6d9c0077"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_IncompatibleKeys(missing_keys=['blocks.0.adapter.down.weight', 'blocks.0.adapter.down.bias', 'blocks.0.adapter.up.weight', 'blocks.0.adapter.up.bias', 'blocks.1.adapter.down.weight', 'blocks.1.adapter.down.bias', 'blocks.1.adapter.up.weight', 'blocks.1.adapter.up.bias', 'blocks.2.adapter.down.weight', 'blocks.2.adapter.down.bias', 'blocks.2.adapter.up.weight', 'blocks.2.adapter.up.bias', 'blocks.3.adapter.down.weight', 'blocks.3.adapter.down.bias', 'blocks.3.adapter.up.weight', 'blocks.3.adapter.up.bias', 'blocks.4.adapter.down.weight', 'blocks.4.adapter.down.bias', 'blocks.4.adapter.up.weight', 'blocks.4.adapter.up.bias', 'blocks.5.adapter.down.weight', 'blocks.5.adapter.down.bias', 'blocks.5.adapter.up.weight', 'blocks.5.adapter.up.bias', 'blocks.6.adapter.down.weight', 'blocks.6.adapter.down.bias', 'blocks.6.adapter.up.weight', 'blocks.6.adapter.up.bias', 'blocks.7.adapter.down.weight', 'blocks.7.adapter.down.bias', 'blocks.7.adapter.up.weight', 'blocks.7.adapter.up.bias', 'blocks.8.adapter.down.weight', 'blocks.8.adapter.down.bias', 'blocks.8.adapter.up.weight', 'blocks.8.adapter.up.bias', 'blocks.9.adapter.down.weight', 'blocks.9.adapter.down.bias', 'blocks.9.adapter.up.weight', 'blocks.9.adapter.up.bias', 'blocks.10.adapter.down.weight', 'blocks.10.adapter.down.bias', 'blocks.10.adapter.up.weight', 'blocks.10.adapter.up.bias', 'blocks.11.adapter.down.weight', 'blocks.11.adapter.down.bias', 'blocks.11.adapter.up.weight', 'blocks.11.adapter.up.bias', 'head.weight', 'head.bias'], unexpected_keys=['pre_logits.fc.bias', 'pre_logits.fc.weight', 'blocks.0.attn.qkv.bias', 'blocks.1.attn.qkv.bias', 'blocks.2.attn.qkv.bias', 'blocks.3.attn.qkv.bias', 'blocks.4.attn.qkv.bias', 'blocks.5.attn.qkv.bias', 'blocks.6.attn.qkv.bias', 'blocks.7.attn.qkv.bias', 'blocks.8.attn.qkv.bias', 'blocks.9.attn.qkv.bias', 'blocks.10.attn.qkv.bias', 'blocks.11.attn.qkv.bias'])\n",
            "number of tunable params (M): 0.23\n"
          ]
        }
      ],
      "source": [
        "import timm\n",
        "\n",
        "\n",
        "######## implement you code here###########\n",
        "class AdapterBlock(Block):\n",
        "  def __init__(self):\n",
        "    super(AdapterBlock, self).__init__(dim = 768, num_heads = 12)\n",
        "    self.adapter = Adapter()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n",
        "    x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))) + self.adapter.forward(x))\n",
        "    return x\n",
        "\n",
        "def set_adapter(model):\n",
        "  for i, _ in enumerate(model.blocks):\n",
        "    adapter_block = AdapterBlock()\n",
        "    model.blocks[i] = adapter_block\n",
        "  # named_parameters = list(model.named_parameters())\n",
        "  # for name, _ in named_parameters:\n",
        "  #   if 'head' not in name:\n",
        "  #     adapter = Adapter()\n",
        "  #     setattr(model, name + '_adapter', adapter)\n",
        "######## implement you code here ###########\n",
        "\n",
        "\n",
        "\n",
        "set_adapter(model=model)\n",
        "\n",
        "# load pre-trained parameter agrain\n",
        "msg = model.load_state_dict(checkpoint_model, strict=False)\n",
        "print(msg)\n",
        "\n",
        "\n",
        "# freeze all but the head\n",
        "for name, p in model.named_parameters():\n",
        "    if name in msg.missing_keys:\n",
        "        p.requires_grad = True\n",
        "    else:\n",
        "        p.requires_grad = False\n",
        "for _, p in model.head.named_parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "n_parameters = 0\n",
        "for n, p in model.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        n_parameters = n_parameters + p.numel()\n",
        "\n",
        "\n",
        "print('number of tunable params (M): %.2f' % (n_parameters / 1.e6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJFZB_A7ddrC"
      },
      "source": [
        "## Parameter-efficient fine-tuning for 1 epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "YgGgENYF0Z-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c87bb64-52e4-4bf7-a5db-5dd605035a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "500\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from timm.data.constants import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
        "import PIL.Image\n",
        "import math\n",
        "\n",
        "_mean = IMAGENET_INCEPTION_MEAN\n",
        "_std = IMAGENET_INCEPTION_STD\n",
        "\n",
        "class RandomResizedCrop(transforms.RandomResizedCrop):\n",
        "    \"\"\"\n",
        "    RandomResizedCrop for matching TF/TPU implementation: no for-loop is used.\n",
        "    This may lead to results different with torchvision's version.\n",
        "    Following BYOL's TF code:\n",
        "    https://github.com/deepmind/deepmind-research/blob/master/byol/utils/dataset.py#L206\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def get_params(img, scale, ratio):\n",
        "        assert isinstance(img, PIL.Image.Image)\n",
        "        # width, height = F._get_image_size(img)\n",
        "        width, height = img.width, img.height\n",
        "        area = height * width\n",
        "\n",
        "        target_area = area * torch.empty(1).uniform_(scale[0], scale[1]).item()\n",
        "        log_ratio = torch.log(torch.tensor(ratio))\n",
        "        aspect_ratio = torch.exp(\n",
        "            torch.empty(1).uniform_(log_ratio[0], log_ratio[1])\n",
        "        ).item()\n",
        "\n",
        "        w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
        "        h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
        "\n",
        "        w = min(w, width)\n",
        "        h = min(h, height)\n",
        "\n",
        "        i = torch.randint(0, height - h + 1, size=(1,)).item()\n",
        "        j = torch.randint(0, width - w + 1, size=(1,)).item()\n",
        "\n",
        "        return i, j, h, w\n",
        "\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR100(\n",
        "    root = 'Cifar100',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        RandomResizedCrop(224, interpolation=3),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=_mean, std=_std)])\n",
        ")\n",
        "\n",
        "test_set = torchvision.datasets.CIFAR100(\n",
        "    root = 'Cifar100',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256, interpolation=3),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=_mean, std=_std)])\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
        "print(len(train_loader))\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "yJDrFISi0Z-j"
      },
      "outputs": [],
      "source": [
        "network = model\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    network = network.cuda()\n",
        "\n",
        "# you may aduject the learning rate or weight deacy here.\n",
        "\n",
        "######### complete the optimizer. ##################\n",
        "optimizer = optim.Adam(network.parameters(), lr=1e-2)\n",
        "######### complete the optimizer. ##################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS_b8oso0Z-j"
      },
      "source": [
        "\n",
        "\n",
        "Build the train loop. You should finish the first epoch training!!!!!!!!!!!!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "w2LF7BMR0Z-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70e78a02-876d-4a0b-d04b-7794c40b9bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [20:38<00:00,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:0  ,  Loss:490.153138  , Train Accuracy:75.154000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "for epoch in range(1):\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    for batch in tqdm.tqdm(train_loader):\n",
        "        images, labels = batch\n",
        "        if torch.cuda.is_available():\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = network(images)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _,prelabels=torch.max(preds,dim=1)\n",
        "        total_correct += (prelabels==labels).sum().item()\n",
        "\n",
        "    accuracy = total_correct/len(train_set)\n",
        "    print(\"Epoch:%d  ,  Loss:%f  , Train Accuracy:%f \"%(epoch, total_loss, accuracy * 100))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}