{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXMX8yuAyl55"
      },
      "source": [
        "\\# Welcome to CS 5242 **Homework 1**\n",
        "\n",
        "ASSIGNMENT DEADLINE ‚è∞ : **23:59 09 Feb 2024**\n",
        "\n",
        "In this assignment, the task is to implement Multi-Layer Perceptron (MLP) for predicting the price of houses in Boston from scratch.\n",
        "\n",
        "Colab is a hosted Jupyter notebook service that requires no setup to use, while providing access free of charge to computing resources including GPUs. In this semester, we will use Colab to run our experiments.\n",
        "\n",
        "\n",
        "### **Grades Policy**\n",
        "\n",
        "We have 10 points for this homework. 15% off per day late, 0 scores if you submit it 7 days after the deadline.\n",
        "\n",
        "### **Cautions**\n",
        "\n",
        "**DO NOT** use external libraries like PyTorch or TensorFlow in your implementation.\n",
        "\n",
        "**DO NOT** copy the code from the internet, e.g. GitHub.\n",
        "\n",
        "Rename the assignment file as \"StuID_Name_assignment-1.ipynb\". e.g., 'A0100000J_Wang-Wenjie_assignment-1.ipynb'. And submit it to Canvas. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Contact**\n",
        "\n",
        "Please feel free to contact us if you have any question about this homework or need any further information.\n",
        "\n",
        "Slack: Ziming Liu\n",
        "\n",
        "TA Email: liuziming@comp.nus.edu.sg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLeZHcOVBp4U"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Start by running the cell below to set up all required software."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIgu_q2HBg-E"
      },
      "outputs": [],
      "source": [
        "!pip install numpy matplotlib scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtXcchT5H2PH"
      },
      "source": [
        "Import the neccesary library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2Yodsn4H6CB"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_california_housing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yrZD7DDExF4"
      },
      "source": [
        "Everything is ready, you can move on and ***Good Luck !*** üòÉ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DwqHhL9CSxs"
      },
      "source": [
        "## Explore Boston Housing Dataset\n",
        "\n",
        "One of the first steps when working with a new data set is exploring.The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The following describes the dataset columns:\n",
        "\n",
        "CRIM - per capita crime rate by town\n",
        "\n",
        "ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "\n",
        "INDUS - proportion of non-retail business acres per town.\n",
        "\n",
        "CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
        "\n",
        "NOX - nitric oxides concentration (parts per 10 million)\n",
        "\n",
        "RM - average number of rooms per dwelling\n",
        "\n",
        "AGE - proportion of owner-occupied units built prior to 1940\n",
        "\n",
        "DIS - weighted distances to five Boston employment centres\n",
        "\n",
        "RAD - index of accessibility to radial highways\n",
        "\n",
        "TAX - full-value property-tax rate per $10,000\n",
        "\n",
        "PTRATIO - pupil-teacher ratio by town\n",
        "\n",
        "B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
        "\n",
        "LSTAT - % lower status of the population\n",
        "\n",
        "MEDV - Median value of owner-occupied homes in $1000's\n",
        "\n",
        "First, we download the dataset from the Internet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cWrRaLFmiQs"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data \\\n",
        "    && cd data \\\n",
        "    && wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data \\\n",
        "    && ls -l ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7z_yLpMyr9u"
      },
      "source": [
        "The next thing to do is to reformat the data. Now we reshape the data into a (num_sample, num_features) array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RZhYDuxm1aC"
      },
      "outputs": [],
      "source": [
        "datafile = 'data/housing.data'\n",
        "data = np.fromfile(datafile, sep=' ')\n",
        "feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "print(data.shape)\n",
        "feature_num = len(feature_names)\n",
        "data = data.reshape([data.shape[0] // feature_num, feature_num])\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuTCeZ0m_Zur"
      },
      "source": [
        "One important step of data processing is to rescale the features. In this case, we use minmax scaling. After rescaling, the range of the data will become 0-1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6KGKdP7JCmc"
      },
      "outputs": [],
      "source": [
        "# rescale data using minmax scaling.\n",
        "def scale_data(X):\n",
        "    X_scaled = None\n",
        "    #########Enter your code here######### (One point)\n",
        "\n",
        "    #########Code end#########\n",
        "    return X_scaled\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jet4OTlkm93o"
      },
      "outputs": [],
      "source": [
        "ratio = 0.8\n",
        "offset = int(data.shape[0] * ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "461Ro4TXnKlO"
      },
      "outputs": [],
      "source": [
        "x = data[:, :-1]\n",
        "y = data[:, -1:]\n",
        "x = scale_data(x)\n",
        "X_train = x[:offset]\n",
        "X_test = x[offset:]\n",
        "Y_train = y[:offset]\n",
        "Y_test = y[offset:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aH3JKg3I2bZ"
      },
      "source": [
        "An easy to quickly get an idea for how your data looks is to examine the shape of the matrix it's stored using the `.shape` attribute of numpy arrays. We see that the shape of `X_train` is `404 x 13`, which tells us there are `404` samples (images) each with dimension `13`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNCf79ShI6H9"
      },
      "outputs": [],
      "source": [
        "print(f\"The shape of the training set is: {X_train.shape[0]} x {X_train.shape[1]}\")\n",
        "print(f\"The shape of the test set is: {X_test.shape[0]} x {X_test.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf5GmYOTzvVI"
      },
      "source": [
        "Now lets take a look at how the samples are represented, we can do this by calling `Y_train[index]` and `X_train[index]` (here I choose `index=0` to look at the very first sample). We first notice `Y_train[0]=24`, meaning this entry is a house worthy of 24. The data type of the numpy array is float64 by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZWVJHTXzzp2"
      },
      "outputs": [],
      "source": [
        "index = 0\n",
        "print(f\"Y_train[{index}]: {Y_train[index]}\")\n",
        "print(X_train[index])\n",
        "print(X_train[index].dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDU7W9FNXix6"
      },
      "source": [
        "We can also check how related the features are with the house price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G0EUVRs2DCh"
      },
      "outputs": [],
      "source": [
        "housedatadf=pd.DataFrame(data=X_train,columns=feature_names[:-1])\n",
        "housedatadf[\"target\"]=Y_train\n",
        "datacor=np.corrcoef(housedatadf.values,rowvar=0)\n",
        "datacor=pd.DataFrame(data=datacor,columns=housedatadf.columns,index=housedatadf.columns)\n",
        "plt.figure(figsize=(15,10))\n",
        "ax=sns.heatmap(datacor,square=True,annot=True,fmt=\".3f\",linewidths=5,cmap=\"YlGnBu\",cbar_kws={\"fraction\":0.046,\"pad\":0.03})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTHgDB8MCguR"
      },
      "source": [
        "## Implementation of Multilayer perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS0HX6A_o5mL"
      },
      "source": [
        "We'll first go through and write the code for each piece of an MLP in generic Python functions. We'll then wrap everything in an `MLP` class, which will allow us to easily access all the MLP functionality in a user friendly manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9XZokeNpnUf"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA9w06GhtWxt"
      },
      "source": [
        "You have **three tasks** in this section.\n",
        "1. You need to implement `__init__` function\n",
        "1. You need to implement `mse_loss` function\n",
        "2. You need to implement `forward` function.\n",
        "3. You need to implement `backward` function.\n",
        "4. You need to implement the main loop of `train` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-ERNpYWpYc4"
      },
      "outputs": [],
      "source": [
        "class MLP(object):\n",
        "    def __init__(self, num_of_weights, learning_rate=0.01):\n",
        "        np.random.seed(42)\n",
        "        self.learning_rate = learning_rate\n",
        "        # As this is a very simple problem, we only need one linear layer here.\n",
        "        # Init a linear layer with weight(using standard normal distribution) and bias(to 0).\n",
        "        #########Enter your code here#########  (One point)\n",
        "\n",
        "\n",
        "        #########Code end#########\n",
        "\n",
        "    def mse_loss(self, y_pred, y):\n",
        "\n",
        "        #########Enter your code here#########  (One point)\n",
        "\n",
        "\n",
        "        #########Code end#########\n",
        "        return loss_\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #########Enter your code here#########  (Two points)\n",
        "\n",
        "\n",
        "        #########Code end#########\n",
        "        return out_\n",
        "\n",
        "    def backward(self, x, y_pred, y):\n",
        "\n",
        "        # Calculate the gradient and update the weight and bias using the gradient.\n",
        "        #########Enter your code here######### (Two points)\n",
        "\n",
        "\n",
        "        #########Code end#########\n",
        "\n",
        "\n",
        "    def train(self, X, Y, num_epoches, batch_size):\n",
        "        n_samples = len(X)\n",
        "        losses = []\n",
        "        for epoch_id in range(num_epoches):\n",
        "            shuffle = np.random.permutation(n_samples)\n",
        "            X_batches = np.array_split(X[shuffle], n_samples / batch_size)\n",
        "            Y_batches = np.array_split(Y[shuffle], n_samples / batch_size)\n",
        "            iter_id = 0\n",
        "            for batch_x, batch_y in zip(X_batches, Y_batches):\n",
        "                #########Enter your code here######### (One point)\n",
        "\n",
        "\n",
        "                #########Code end#########\n",
        "                print('Epoch {:3d} / iter {:3d}, loss={:4f}'.format(epoch_id + 1, iter_id, loss))\n",
        "                iter_id += 1\n",
        "        return losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrUiDpPynrLF"
      },
      "source": [
        "Great, now let's give this a try. Let's create a really simple MLP. We'll train with a `batch_size=100` for `epochs=50` and a learning rate `lr=0.1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkkmE_mQpb4j"
      },
      "outputs": [],
      "source": [
        "network = MLP(13, 0.1)\n",
        "losses = network.train(X_train, Y_train, num_epoches=50, batch_size=100)\n",
        "plot_x = np.arange(len(losses))\n",
        "plot_y = np.array(losses)\n",
        "plt.plot(plot_x, plot_y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzZ3aAJabLwr"
      },
      "source": [
        "Now we can check the performance on the test set!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHTVpDHzbMWC"
      },
      "outputs": [],
      "source": [
        "Y_pred=network.forward(X_test)\n",
        "# mse=mean_absolute_error(y_test,pre_y)\n",
        "Y_pred = np.squeeze(Y_pred)\n",
        "Y_test = np.squeeze(Y_test)\n",
        "index=np.argsort(Y_test)\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(np.arange(len(Y_test)),Y_test[index],\"r\",label=\"original y\")\n",
        "plt.scatter(np.arange(len(Y_test)),Y_pred[index],s=3,c=\"b\",label=\"prediction\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.grid()\n",
        "plt.xlabel(\"index\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9rgEKrTCseN"
      },
      "source": [
        "## Discussion (2 points)\n",
        "\n",
        "In this section, you are free to choose some of the following directions to explore, and try to summarize some patterns and conclusions.\n",
        "\n",
        "* Try to use different learning rates and generalize the impact of learning rate on training.\n",
        "* Try to use different ways of weight initialization and explore the impact on the convergence and final classification performance.\n",
        "* Try to use a portion of the data for training (e.g., 10%, 20%, 50%) to explore the training convergence and final classification performance with different amounts of data.\n",
        "\n",
        "> **NOTE:** *Good Disscusion* include experimental setup, presentation of experimental results including visualization, analysis and interpretation of phenomena, and summary of conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpuUw__XX2BE"
      },
      "outputs": [],
      "source": [
        "# === Complete the code (2')\n",
        "\n",
        "# === Complete the code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq6eF7KSX7oS"
      },
      "source": [
        "Write down your analysis and conclusions:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Voo4eCfvcyKO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oLeZHcOVBp4U",
        "5y_Q4zdevADO"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
