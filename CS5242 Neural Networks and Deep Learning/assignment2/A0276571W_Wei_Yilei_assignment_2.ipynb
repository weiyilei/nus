{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXMX8yuAyl55"
      },
      "source": [
        "# Welcome to CS 5242 **Assignment 2**\n",
        "\n",
        "ASSIGNMENT DEADLINE ‚è∞ : ** 3 March 2024**\n",
        "\n",
        "In this assignment, we have three parts:\n",
        "1. Implement some operations in CNNs from scratch *(2 Points)*\n",
        "2. Implement a simple CNN and train on MNIST using PyTorch  *(4 Points)*\n",
        "3. Implement a VGG network with PyTorch *(4 Points)*\n",
        "\n",
        "Colab is a hosted Jupyter notebook service that requires no setup to use, while providing access free of charge to computing resources including GPUs. In this semester, we will use Colab to run our experiments.\n",
        "1. Login Google Colab https://colab.research.google.com/\n",
        "2. In this assignment, We **need GPU** to training the CNN model. You may need to **choose GPU in Runtime -> Change runtime type -> Hardware accerator**\n",
        "![Alt text](image.png)\n",
        "\n",
        "\n",
        "### **Grades Policy**\n",
        "\n",
        "We have 10 points for this homework. 15% off per day late, 0 scores if you submit it 7 days after the deadline.\n",
        "\n",
        "### **Cautions**\n",
        "\n",
        "**DO NOT** copy the code from the internet, e.g. GitHub.\n",
        "---\n",
        "\n",
        "**DO NOT** use any LLMs to write the code, e.g. ChatGPT.\n",
        "---\n",
        "\n",
        "### **Contact**\n",
        "\n",
        "Please feel free to contact us if you have any question about this homework or need any further information.\n",
        "\n",
        "Slack: Wangbo Zhao\n",
        "\n",
        "\n",
        "> If you have not join the slack group, you can click [here](https://join.slack.com/t/cs5242-2024spring/shared_invite/zt-2cw3jgqab-wFhoaIVa4RIX4fCZ_k~vjQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLeZHcOVBp4U"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Start by running the cell below to set up all required software."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIgu_q2HBg-E",
        "outputId": "eb5a995a-cc1a-4ac0-876e-c7de1b2f8de3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy matplotlib\n",
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtXcchT5H2PH"
      },
      "source": [
        "Import the neccesary library and fix seed for Python, NumPy and PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2Yodsn4H6CB",
        "outputId": "f7c104ee-47b8-4c68-dccd-bbcc7006c22f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e84383626f0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTpFBLKSkKI0"
      },
      "source": [
        "Now let's setup the GPU environment. The colab provides a free GPU to use. Do as follows:\n",
        "\n",
        "- Runtime -> Change Runtime Type -> select `GPU` in Hardware accelerator\n",
        "- Click `connect` on the top-right\n",
        "\n",
        "After connecting to one GPU, you can check its status using `nvidia-smi` command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES8KOxziiYky",
        "outputId": "e2366adb-a710-478a-a112-bb083de1fa01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yrZD7DDExF4"
      },
      "source": [
        "Everything is ready, you can move on and ***Good Luck !*** üòÉ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm1f362vdRgF"
      },
      "source": [
        "## Implement some operations in CNNs from scratch\n",
        "\n",
        "In this section, you need to implement some operations commonly used in CNNs, including convolution and pooling.\n",
        "\n",
        "You need to compare the computational results of your implemented version with those of Pytorch, expecting that the error between the correct implementation and pytorch will be very small.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV3DXc2jgeg7"
      },
      "source": [
        "### Step 1\n",
        "Given a 32x32 pixels, 3 channels input, get a torch tensor with torch.randn()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3UxGJxTegq9O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c33aed81-c4d0-4eb4-b65d-554ae210b163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-1.1258, -1.1524, -0.2506,  ...,  1.5863,  0.9463, -0.8437],\n",
            "          [-0.6136,  0.0316, -0.4927,  ..., -1.2341,  1.8197, -0.5515],\n",
            "          [-0.5692,  0.9200,  1.1108,  ..., -0.9565,  0.0335,  0.7101],\n",
            "          ...,\n",
            "          [ 1.0166,  1.2868,  2.0820,  ...,  0.8161, -0.5711, -0.1195],\n",
            "          [-0.4274,  0.8143, -1.4121,  ..., -0.1394, -0.3677, -0.4574],\n",
            "          [-1.2945,  0.7012, -1.9098,  ...,  0.5374,  1.0826, -1.7105]],\n",
            "\n",
            "         [[-1.0841, -0.1287, -0.6811,  ..., -0.9825,  0.7184,  0.4402],\n",
            "          [-0.5619,  0.6640, -2.1033,  ..., -0.7821, -2.1407,  0.3337],\n",
            "          [-1.1230,  0.6210, -0.8764,  ...,  0.9159,  0.2990,  0.1771],\n",
            "          ...,\n",
            "          [ 2.2746, -0.9119,  0.5105,  ...,  0.4876, -0.9265, -0.5748],\n",
            "          [ 0.7300, -0.9287,  0.1743,  ..., -0.7073, -0.8813, -0.5895],\n",
            "          [-0.8363, -1.8354,  0.4765,  ..., -0.3812, -1.6687,  1.0869]],\n",
            "\n",
            "         [[ 0.6657,  0.8847,  0.4671,  ...,  0.7709, -0.8416,  1.7962],\n",
            "          [ 0.1924, -0.1777,  0.3214,  ..., -1.1616, -0.5921,  0.7457],\n",
            "          [-1.1870, -0.8221,  0.6051,  ..., -0.1906,  0.2511,  1.3542],\n",
            "          ...,\n",
            "          [ 1.9324, -0.5826, -1.3121,  ...,  0.2871,  0.2620, -0.3582],\n",
            "          [ 2.8424, -0.6401, -0.5874,  ...,  0.4994, -1.5602,  1.1315],\n",
            "          [-0.0504,  0.5482, -1.2351,  ..., -0.6380, -1.1714, -0.8415]]],\n",
            "\n",
            "\n",
            "        [[[-2.1179, -0.0779, -0.1979,  ...,  0.8558, -0.2028, -0.3681],\n",
            "          [-0.6534, -0.8922, -0.2236,  ...,  0.0599, -0.9237, -0.1253],\n",
            "          [-0.5604,  1.1514,  0.2366,  ..., -1.4330, -0.2642,  0.3111],\n",
            "          ...,\n",
            "          [ 1.2874,  0.9561, -1.9727,  ...,  1.0564, -0.1504,  0.7420],\n",
            "          [ 0.7272, -0.2612,  0.0124,  ..., -1.0294, -1.8653, -0.7406],\n",
            "          [ 0.9192, -0.5652, -0.7208,  ..., -3.0357, -1.7288,  0.6020]],\n",
            "\n",
            "         [[ 1.9476,  1.0077, -0.1007,  ..., -1.5322,  0.0142, -0.3296],\n",
            "          [ 0.7450, -0.5086, -0.5947,  ..., -0.1173, -0.6841,  0.5988],\n",
            "          [-0.2579, -1.0667, -0.7595,  ...,  1.4624, -0.0423, -1.2064],\n",
            "          ...,\n",
            "          [ 0.0664, -0.0293,  0.6167,  ...,  0.0214, -0.7314, -0.9386],\n",
            "          [-1.5552, -0.7902, -0.2326,  ...,  0.0857,  0.7980,  0.8385],\n",
            "          [-1.4122, -0.5026, -1.0330,  ..., -2.3026, -1.8670, -0.1782]],\n",
            "\n",
            "         [[ 0.3533, -0.1317, -1.6393,  ..., -1.5319, -0.4684,  0.2686],\n",
            "          [-1.1515, -1.8388, -2.2352,  ..., -2.1816, -0.7663, -0.1473],\n",
            "          [ 1.2842,  0.6412, -1.0515,  ..., -1.4651, -2.6220,  1.0813],\n",
            "          ...,\n",
            "          [-0.2785,  0.6627, -0.2764,  ..., -1.7816,  1.5874, -1.2078],\n",
            "          [-0.7160, -2.0093, -0.8437,  ..., -1.2968, -0.8429,  0.9080],\n",
            "          [-1.3241,  1.9042, -0.4646,  ...,  0.3333, -0.2713,  0.0072]]]])\n",
            "torch.Size([2, 3, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 2\n",
        "c = 3\n",
        "h = 32\n",
        "w = 32\n",
        "x = torch.randn(batch_size, c, h, w)\n",
        "print(x)\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxnlbBnFw9wB"
      },
      "source": [
        "### Step 2\n",
        "We first implement these operations with Pytorch so that we can compare the computational results of our implemented version with those of original pytorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OLQGhRbJgpIZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Build a max pooling layer torch_max_pool with Pytorch. The kernel size of the pooling is 2, the stride is 2, and there is not any padding.\n",
        "torch_max_pool = nn.MaxPool2d(kernel_size=2,\n",
        "                              stride=2,\n",
        "                              padding=0)\n",
        "\n",
        "# 2. Build a average pooling layer torch_avg_pool with Pytorch. The kernel size of the pooling is 2, the stride is 1. The padding shoulbd be set to 1.\n",
        "torch_avg_pool = nn.AvgPool2d(kernel_size=2,\n",
        "                              stride=1,\n",
        "                              padding=1)\n",
        "\n",
        "# 3.Build a 2D convolutional layer torch_conv with Pytorch. The kernel size of the convolution is 3. Stride is 1. The input channel and output channel should be set to 3 and 64, respectively. We use zero padding to keep the spatial size of the output feature.\n",
        "torch_conv = nn.Conv2d(in_channels=3,\n",
        "                       out_channels=64,\n",
        "                       kernel_size=3,\n",
        "                       stride=1,\n",
        "                       padding=1)\n",
        "\n",
        "# 2D batchnorm with channel=3\n",
        "torch_norm = nn.BatchNorm2d(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PBzDAo2rgwmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afea062e-0886-43bf-e956-5cc95bf98f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 16, 16])\n",
            "torch.Size([2, 3, 33, 33])\n",
            "torch.Size([2, 64, 32, 32])\n",
            "torch.Size([2, 3, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "torch_max_pool_out = torch_max_pool(x)\n",
        "print(torch_max_pool_out.shape)\n",
        "\n",
        "torch_avg_pool_out = torch_avg_pool(x)\n",
        "print(torch_avg_pool_out.shape)\n",
        "\n",
        "torch_conv_out = torch_conv(x)\n",
        "print(torch_conv_out.shape)\n",
        "\n",
        "torch_norm_out = torch_norm(x)\n",
        "print(torch_norm_out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6YBRP6Qgylg"
      },
      "source": [
        "### Step 3\n",
        "\n",
        "Implement these operations from scratch. Output your tensors as \"my_xxx_out\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CsO5I40fgzWY"
      },
      "outputs": [],
      "source": [
        "def my_max_pool(x, kernel_size, stride, padding):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: torch tensor with size (N, C_in, H_in, W_in),\n",
        "        kernel_size: size of the window to take a max over,\n",
        "        stride: stride of the window,\n",
        "        padding: implicit zero padding to be added on both sides,\n",
        "\n",
        "    Return:\n",
        "        y: torch tensor of size (N, C_out, H_out, W_out).\n",
        "    \"\"\"\n",
        "\n",
        "    y = None\n",
        "    # === Complete the code (0.5')\n",
        "\n",
        "    N, C_in, H_in, W_in = x.shape\n",
        "    H_out = (H_in - kernel_size + 2*padding)//stride + 1\n",
        "    W_out = (W_in - kernel_size + 2*padding)//stride + 1\n",
        "\n",
        "    pad = (padding,padding,padding,padding)\n",
        "    padded_x = F.pad(x, pad, mode='constant', value=0)\n",
        "\n",
        "    y = torch.zeros((N, C_in, H_out, W_out))\n",
        "\n",
        "    for i in range(N):\n",
        "        for j in range(C_in):\n",
        "            for k in range(0, H_in - kernel_size + 1, stride):\n",
        "            # for k in range(0, 1):\n",
        "                for l in range(0, W_in - kernel_size + 1, stride):\n",
        "                # for l in range(0, 1):\n",
        "                    temp = padded_x[i, j, k:k+kernel_size, l:l+kernel_size]\n",
        "                    # print(temp)\n",
        "                    # print(temp.shape)\n",
        "                    y[i, j, k//stride, l//stride] = torch.max(temp)\n",
        "    # print(y)\n",
        "    # print(y.shape)\n",
        "\n",
        "    # === Complete the code\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nGbn6oQcg4pM"
      },
      "outputs": [],
      "source": [
        "def my_avg_pool(x, kernel_size, stride, padding):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: torch tensor with size (N, C_in, H_in, W_in),\n",
        "        kernel_size: size of the window,\n",
        "        stride: stride of the window,\n",
        "        padding: implicit zero padding to be added on both sides,\n",
        "\n",
        "    Return:\n",
        "        y: torch tensor of size (N, C_out, H_out, W_out).\n",
        "    \"\"\"\n",
        "\n",
        "    y = None\n",
        "    # === Complete the code (0.5')\n",
        "\n",
        "    N, C_in, H_in, W_in = x.shape\n",
        "    H_out = (H_in - kernel_size + 2*padding)//stride + 1\n",
        "    W_out = (W_in - kernel_size + 2*padding)//stride + 1\n",
        "\n",
        "    pad = (padding,padding,padding,padding)\n",
        "    padded_x = F.pad(x, pad, mode='constant', value=0)\n",
        "    # print(padded_x)\n",
        "    # print(padded_x.shape)\n",
        "\n",
        "    y = torch.zeros((N, C_in, H_out, W_out), dtype=torch.float64)\n",
        "\n",
        "    for i in range(N):\n",
        "        for j in range(C_in):\n",
        "            for k in range(0, H_in - kernel_size + 1 + 2*padding, stride):\n",
        "              # for k in range(0, 1):\n",
        "                for l in range(0, W_in - kernel_size + 1 + 2*padding, stride):\n",
        "                  # for l in range(0, 1):\n",
        "                    temp = padded_x[i, j, k:k+kernel_size, l:l+kernel_size].to(dtype=torch.float64)\n",
        "                    # print(temp)\n",
        "                    # print(temp.shape)\n",
        "                    average = torch.mean(temp, dtype=torch.float64)\n",
        "                    y[i, j, k//stride, l//stride] = average.to(dtype=torch.float64)\n",
        "    # print(y)\n",
        "    # print(f\"{y[0,0,0,0]:.10f}\")\n",
        "    # print(y.shape)\n",
        "\n",
        "    # === Complete the code\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9gsDytvKg5c1"
      },
      "outputs": [],
      "source": [
        "def my_conv(x, in_channels, out_channels, kernel_size, stride, padding, weight, bias):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: torch tensor with size (N, C_in, H_in, W_in),\n",
        "        in_channels: number of channels in the input image, it is C_in;\n",
        "        out_channels: number of channels produced by the convolution;\n",
        "        kernel_size: size of onvolving kernel,\n",
        "        stride: stride of the convolution,\n",
        "        padding: implicit zero padding to be added on both sides of each dimension,\n",
        "\n",
        "    Return:\n",
        "        y: torch tensor of size (N, C_out, H_out, W_out)\n",
        "    \"\"\"\n",
        "\n",
        "    y = None\n",
        "    # === Complete the code (0.5')\n",
        "\n",
        "    N, C_in, H_in, W_in = x.shape\n",
        "    # outchannels, in_channels, kernel_size, kernel_size = weight.shape\n",
        "    H_out = (H_in - kernel_size + 2*padding)//stride + 1\n",
        "    W_out = (W_in - kernel_size + 2*padding)//stride + 1\n",
        "\n",
        "    pad = (padding,padding,padding,padding)\n",
        "    padded_x = F.pad(x, pad, mode='constant', value=0)\n",
        "    # print(padded_x)\n",
        "    # print(padded_x.shape)\n",
        "\n",
        "    y = torch.zeros((N, out_channels, H_out, W_out), dtype=torch.float64)\n",
        "\n",
        "    for i in range(N):\n",
        "        for j in range(out_channels):\n",
        "            for k in range(0, H_in - kernel_size + 1 + 2*padding, stride):\n",
        "            # for k in range(0,1):\n",
        "                for l in range(0, W_in - kernel_size + 1 + 2*padding, stride):\n",
        "                # for l in range(0,1):\n",
        "                    temp = padded_x[i, :, k:k+kernel_size, l:l+kernel_size].to(dtype=torch.float64)\n",
        "                    # print(temp)\n",
        "                    # print(temp.shape)\n",
        "                    # print(weight[j])\n",
        "                    # print(weight[j].shape)\n",
        "                    result = (torch.sum(temp * weight[j]) + bias[j]).to(dtype=torch.float64)\n",
        "                    # Â∞ÜÁªìÊûúÂÜôÂÖ•ËæìÂá∫Âº†Èáè\n",
        "                    y[i, j, k//stride, l//stride] = result.to(dtype=torch.float64)\n",
        "    # print(y)\n",
        "    # print(f\"{y[0,0,0,0]:.10f}\")\n",
        "    # print(y.shape)\n",
        "\n",
        "    # === Complete the code\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8sX0oRyTg-m6"
      },
      "outputs": [],
      "source": [
        "def my_batchnorm(x, num_features, eps=1e-5):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: torch tensor with size (N, C, H, W),\n",
        "        num_features: number of features in the input tensor, it is C;\n",
        "        eps: a value added to the denominator for numerical stability. Default: 1e-5\n",
        "\n",
        "    Return:\n",
        "        y: torch tensor of size (N, C, H, W)\n",
        "    \"\"\"\n",
        "\n",
        "    y = torch.empty_like(x)\n",
        "    # === Complete the code (0.5')\n",
        "\n",
        "    mean = torch.mean(x, dim=(0, 2, 3), keepdim=True)\n",
        "    # print(mean)\n",
        "    # print(mean.shape)\n",
        "    var = torch.var(x, dim=(0, 2, 3), keepdim=True, unbiased=True)\n",
        "    # print(var)\n",
        "    # print(var.shape)\n",
        "\n",
        "    batch_norm = ((x - mean) / torch.sqrt(var + eps))\n",
        "    # print(batch_norm)\n",
        "    # print(batch_norm.shape)\n",
        "\n",
        "    y = batch_norm\n",
        "    # print(y)\n",
        "    # print(f\"{y[0,0,0,0]:.10f}\")\n",
        "    # print(y.shape)\n",
        "\n",
        "    # === Complete the code\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dMnKzeVuhGxu"
      },
      "outputs": [],
      "source": [
        "my_max_pool_out = my_max_pool(x, kernel_size=2, stride=2, padding=0)\n",
        "my_avg_pool_out = my_avg_pool(x, kernel_size=2, stride=1, padding=1)\n",
        "my_conv_out = my_conv(x,\n",
        "                      in_channels=3,\n",
        "                      out_channels=64,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1,\n",
        "                      weight=torch_conv.weight.data,\n",
        "                      bias=torch_conv.bias.data)\n",
        "my_norm_out = my_batchnorm(x, num_features=3, eps=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO-EHT7wm7Dk"
      },
      "source": [
        "### Step 4\n",
        "\n",
        "Compare and show that \"torch_xxx_out\" and \"my_xxx_out\" are equal up to small numerical errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXnNfKKJhOAi",
        "outputId": "6923803f-846b-4752-e386-09fcf95e1bfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.)\n",
            "tensor(3.8468e-16, dtype=torch.float64)\n",
            "tensor(3.0318e-15, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
            "tensor(5.9602e-08, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(F.mse_loss(my_max_pool_out, torch_max_pool_out))\n",
        "print(F.mse_loss(my_avg_pool_out, torch_avg_pool_out))\n",
        "print(F.mse_loss(my_conv_out, torch_conv_out))\n",
        "print(F.mse_loss(my_norm_out, torch_norm_out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hHxuCzkmhP4l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJFZB_A7ddrC"
      },
      "source": [
        "## Implement a simple CNN and train it on MNIST using PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q_DvmeGm_41"
      },
      "source": [
        "### Step 1\n",
        "Create datasets. The MNIST data set is composed of handwritten digit images and digit labels from 0 to 9. It consists of 60,000 training samples and 10,000 test samples. Each sample is a 28 * 28 pixel grayscale handwritten digit image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFdIb6TAm_41",
        "outputId": "2ce24e96-4073-441e-b646-63a889df4248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26421880/26421880 [00:01<00:00, 18876325.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29515/29515 [00:00<00:00, 344045.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4422102/4422102 [00:00<00:00, 6284542.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5148/5148 [00:00<00:00, 4781283.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "train_set = torchvision.datasets.FashionMNIST(\n",
        "    root = 'FashionMNIST/',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "\n",
        "test_set = torchvision.datasets.FashionMNIST(\n",
        "    root = 'FashionMNIST/',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-fU0hWpm_41"
      },
      "source": [
        "### Step 2\n",
        "Create the model.\n",
        "You can build a simple convolutional neural network to conduct the classification. You may refine the architecture based on the accuracy. You can also try different learning rates.\n",
        "**The test accuracy should achieve 85%.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HY8BxJv5m_41"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network,self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(20 * 7 * 7, 50)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        t = self.conv1(input)\n",
        "        t = self.relu1(t)\n",
        "        t = self.max_pool1(t)\n",
        "        t = self.conv2(t)\n",
        "        t = self.relu2(t)\n",
        "        t = self.max_pool2(t)\n",
        "        # print(t)\n",
        "        # print(t.shape)\n",
        "        t = t.view(-1, 20 * 7 * 7)\n",
        "        t = self.fc1(t)\n",
        "        t = self.relu3(t)\n",
        "        t = self.fc2(t)\n",
        "\n",
        "        return t\n",
        "\n",
        "network = Network()\n",
        "if torch.cuda.is_available():\n",
        "    network = network.cuda()\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Veax5Adhm_42"
      },
      "source": [
        "### Step 3\n",
        "\n",
        "Build the train and test loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJHtdGvmm_42",
        "outputId": "59326639-5726-4f47-bf19-af4344006eae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:0  ,  Loss:376.303199  , Train Accuracy:77.586667 \n",
            "Epoch:1  ,  Loss:245.289699  , Train Accuracy:85.438333 \n",
            "Epoch:2  ,  Loss:217.041146  , Train Accuracy:87.180000 \n",
            "Epoch:3  ,  Loss:199.453494  , Train Accuracy:88.190000 \n",
            "Epoch:4  ,  Loss:187.056528  , Train Accuracy:88.885000 \n",
            "Epoch:5  ,  Loss:176.970339  , Train Accuracy:89.371667 \n",
            "Epoch:6  ,  Loss:167.991592  , Train Accuracy:89.856667 \n",
            "Epoch:7  ,  Loss:160.161007  , Train Accuracy:90.320000 \n",
            "Epoch:8  ,  Loss:153.383451  , Train Accuracy:90.668333 \n",
            "Epoch:9  ,  Loss:146.680733  , Train Accuracy:91.008333 \n",
            "Test Accuracy:  89.58\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    for batch in train_loader:\n",
        "        images, labels = batch\n",
        "        if torch.cuda.is_available():\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = network(images)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _,prelabels=torch.max(preds,dim=1)\n",
        "        total_correct += (prelabels==labels).sum().item()\n",
        "    accuracy = total_correct/len(train_set)\n",
        "    print(\"Epoch:%d  ,  Loss:%f  , Train Accuracy:%f \"%(epoch, total_loss, accuracy * 100))\n",
        "\n",
        "\n",
        "correct=0\n",
        "total=0\n",
        "network.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        imgs,labels=batch\n",
        "        if torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        preds=network(imgs)\n",
        "        _,prelabels=torch.max(preds,dim=1)\n",
        "        #print(prelabels.size())\n",
        "        total=total+labels.size(0)\n",
        "        correct=correct+int((prelabels==labels).sum())\n",
        "    #print(total)\n",
        "    accuracy=correct / total\n",
        "    print(\"Test Accuracy: \", accuracy * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmrPknkpm_42"
      },
      "source": [
        "# Implement a VGG network with PyTorch\n",
        "VGG is a type of CNN (Convolutional Neural Network) that was considered to be one of the best computer vision models in 2015.\n",
        "https://arxiv.org/abs/1409.1556\n",
        "\n",
        "Here is the configuration of the network from its paper. Now, you need to implement **Config C** it with Pytorch.\n",
        "\n",
        "![Alt text](image-1.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PBm8XyyVm_42"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(VGG, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.relu6 = nn.ReLU()\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        self.conv7 = nn.Conv2d(256, 256, kernel_size=1, padding=1)\n",
        "        self.relu7 = nn.ReLU()\n",
        "        self.bn7 = nn.BatchNorm2d(256)\n",
        "        self.max_pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv8 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.relu8 = nn.ReLU()\n",
        "        self.bn8 = nn.BatchNorm2d(512)\n",
        "        self.conv9 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.relu9 = nn.ReLU()\n",
        "        self.bn9 = nn.BatchNorm2d(512)\n",
        "        self.conv10 = nn.Conv2d(512, 512, kernel_size=1, padding=1)\n",
        "        self.relu10 = nn.ReLU()\n",
        "        self.bn10 = nn.BatchNorm2d(512)\n",
        "        self.max_pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv11 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.relu11 = nn.ReLU()\n",
        "        self.bn11 = nn.BatchNorm2d(512)\n",
        "        self.conv12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.relu12 = nn.ReLU()\n",
        "        self.bn12 = nn.BatchNorm2d(512)\n",
        "        self.conv13 = nn.Conv2d(512, 512, kernel_size=1, padding=1)\n",
        "        self.relu13 = nn.ReLU()\n",
        "        self.bn13 = nn.BatchNorm2d(512)\n",
        "        self.max_pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(512 * 7 * 7, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, 1000)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, image):\n",
        "\n",
        "        x = self.bn1(self.relu1(self.conv1(image)))\n",
        "        x = self.bn2(self.relu2(self.conv2(x)))\n",
        "        x = self.max_pool1(x)\n",
        "\n",
        "        x = self.bn3(self.relu3(self.conv3(x)))\n",
        "        x = self.bn4(self.relu4(self.conv4(x)))\n",
        "        x = self.max_pool2(x)\n",
        "\n",
        "        x = self.bn5(self.relu5(self.conv5(x)))\n",
        "        x = self.bn6(self.relu6(self.conv6(x)))\n",
        "        x = self.bn7(self.relu7(self.conv7(x)))\n",
        "        x = self.max_pool3(x)\n",
        "\n",
        "        x = self.bn8(self.relu8(self.conv8(x)))\n",
        "        x = self.bn9(self.relu9(self.conv9(x)))\n",
        "        x = self.bn10(self.relu10(self.conv10(x)))\n",
        "        x = self.max_pool4(x)\n",
        "\n",
        "        x = self.bn11(self.relu11(self.conv11(x)))\n",
        "        x = self.bn12(self.relu12(self.conv12(x)))\n",
        "        x = self.bn13(self.relu13(self.conv13(x)))\n",
        "        x = self.max_pool5(x)\n",
        "\n",
        "        x = x.view(-1, 512 * 7 * 7)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF5hmdrYm_42"
      },
      "source": [
        "Then, please calculate the number of parameters and FLOPs (Floating point operations) of **Config C**.\n",
        "You can only consider the FLOPs of the convolution and FC in **Config C**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolution layer:\n",
        "\n",
        "conv1: 3x3x3x64=1728 parameters, 3x3x3x64x224x224=86704128 FLOPs\n",
        "\n",
        "conv2: 3x3x64x64=36864 parameters, 3x3x64x64x224x224=1849688064 FLOPs\n",
        "\n",
        "conv3: 3x3x64x128=73728 parameters, 3x3x64x128x112x112=924844032 FLOPs\n",
        "\n",
        "conv4: 3x3x128x128=147456 parameters, 3x3x128x128x112x112=1849688064 FLOPs\n",
        "\n",
        "conv5: 3x3x128x256=294912 parameters, 3x3x128x256x56x56=924844032 FLOPs\n",
        "\n",
        "conv6: 3x3x256x256=589824 parameters, 3x3x256x256x56x56=1849688064 FLOPs\n",
        "\n",
        "conv7: 1x1x256x256=65536 parameters, 1x1x256x256x56x56=205520896 FLOPs\n",
        "\n",
        "conv8: 3x3x256x512=1179648 parameters, 3x3x256x512x28x28=924844032 FLOPs\n",
        "\n",
        "conv9: 3x3x512x512=2359296 parameters, 3x3x512x512x28x28=1849688064 FLOPs\n",
        "\n",
        "conv10: 1x1x512x512=262144 parameters, 1x1x512x512x28x28=205520896 FLOPs\n",
        "\n",
        "conv11: 3x3x512x512=2359296 parameters, 3x3x512x512x14x14=462422016 FLOPs\n",
        "\n",
        "conv12: 3x3x512x512=2359296 parameters, 3x3x512x512x14x14=462422016 FLOPs\n",
        "\n",
        "conv13: 1x1x512x512=262144 parameters, 1x1x512x512x14x14=51380224 FLOPs\n",
        "\n",
        "FC layer:\n",
        "\n",
        "fc1: (512x7x7 + 1) x 4096=102764544 parameters, 512x7x7x4096=102760448 FLOPs\n",
        "\n",
        "fc2: (4096 + 1) x 4096=16781312 parameters, 4096x4096=16777216 FLOPs\n",
        "\n",
        "fc3: (4096 + 1) x 1000=4097000 parameters, 4096x1000=4096000 FLOPs\n",
        "\n",
        "Total parameters: 133634728, if we need to take bias into account, then the number of parameters is 133638952.\n",
        "\n",
        "Total FLOPs: 11770888192\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i5gbNDccBeaC"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}