{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMsrySAKzEq6"
      },
      "source": [
        "\\# Welcome to CS 5242 **Homework 4**\n",
        "\n",
        "ASSIGNMENT DEADLINE ‚è∞ : **23:59 04 April 2024**\n",
        "\n",
        "In this assignment, we will delve into **different generation methods of GPT**. To be specific, you will practice different text generation methods using the powerful [OPT](https://arxiv.org/abs/2205.01068) language model based on [transformers](https://huggingface.co/docs/transformers/en/index) package. You will implement two generation techniques: greedy search, beam search and sampling.\n",
        "\n",
        "Helpful material: https://huggingface.co/blog/how-to-generate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the OPT model and tokenizer\n",
        "# To save memory, we only use opt-350m\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "\n",
        "prompt = \"In this assignment, we will delve into different generation methods of GPT. To be specific, you will\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "print(f\"tokenized input: {input_ids}, shape: {input_ids.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye5nZ6OVI6qg"
      },
      "source": [
        "### Task 1:  Greedy Search\n",
        "\n",
        "Greedy search is a simple and straightforward method for text generation. At each step, it selects the next token with the highest probability according to the language model's output. This approach is called \"greedy\" because it greedily chooses the most likely token at each time step without considering future consequences. Greedy search is fast but may lead to repetitive or monotonous results.\n",
        "\n",
        "Greedy search is a simple method where at each time step, the word with the highest probability is chosen as the next predicted word, until an end token is generated or the maximum length is reached.\n",
        "\n",
        "It can be represented by the formula:\n",
        "\n",
        "$ w_t = \\arg\\max P(w_t | w_{<t}) $\n",
        "\n",
        "Here, $ w_t $ is the predicted word at time step $ t $, and $ P(w_t | w_{<t}) $ is the probability of predicting $ w_t $ given the previous word sequence $ w_{<t} $.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def greedy_search_generation(input_ids, model) -> str:\n",
        "    outputs = input_ids\n",
        "    \n",
        "    # =========================\n",
        "    # Your code starts here (3 points)\n",
        "    # outputs should be a torch Tensor of shape [1, n]\n",
        "    # where n is the length of the generated text\n",
        "    # and the elements are the token ids of the generated text\n",
        "    # \n",
        "    # You should not use any external library like transformers's .generate().\n",
        "    # Only torch is allowed.\n",
        "    # =========================\n",
        "    \n",
        "    # =========================\n",
        "    # Your code ends here\n",
        "    # =========================\n",
        "\n",
        "    return outputs\n",
        "\n",
        "generated_ids = greedy_search_generation(input_ids, model)[0]\n",
        "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "print(f\"prompt: {prompt}\\ngenerated_text: {generated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQQ3k-DBFGEb"
      },
      "source": [
        "### Task 2: Beam Search\n",
        "\n",
        "\n",
        "Beam search is a more complex method that considers multiple candidate words rather than just a single word. At each time step, it keeps track of the top $ k $ most likely partial sequences (known as the beam width), and then expands these partial sequences based on their scores. Finally, it selects the sequence with the highest score from these expanded sequences. Beam search can provide more diverse results but might still result in repetitive outputs.\n",
        "\n",
        "Here's the basic algorithm for beam search:\n",
        "\n",
        "1. **Initialization**:\n",
        "   - Start with an initial input sequence (usually a special token indicating the beginning of a sequence).\n",
        "   - Initialize a set of beams, each containing the initial sequence and its probability score.\n",
        "\n",
        "2. **Expansion**:\n",
        "   - For each beam, generate the next token in the sequence using the GPT model.\n",
        "   - Calculate the conditional probability of each possible next token given the current partial sequence.\n",
        "   - Expand each beam by appending each possible next token to the partial sequence, along with its updated probability score.\n",
        "\n",
        "3. **Selection**:\n",
        "   - Select the top-k beams with the highest probability scores, where k is the beam width.\n",
        "   - Discard the remaining beams.\n",
        "\n",
        "4. **Termination**:\n",
        "   - If the generated token is an end-of-sequence token or a maximum sequence length is reached, terminate those beams.\n",
        "   - Repeat steps 2-4 until all remaining beams either terminate or reach the maximum sequence length.\n",
        "\n",
        "The beam search algorithm can be expressed mathematically as follows:\n",
        "\n",
        "- Let $ S_t^b $ denote the partial sequence for beam $ b $ at time step $ t $.\n",
        "- Let $ P(S_t^b) $ denote the probability of the partial sequence $ S_t^b $ up to time step $ t $.\n",
        "- Let $ P(w_t | S_t^b) $ denote the conditional probability of the next token $ w_t $ given the partial sequence $ S_t^b $ at time step $ t $.\n",
        "- Let $ K $ denote the beam width.\n",
        "\n",
        "The beam search algorithm can be summarized with the following formulas:\n",
        "\n",
        "1. **Initialization**:\n",
        "   $ S_1^1 = \\text{[BOS]} $\n",
        "   $ P(S_1^1) = 1 $\n",
        "\n",
        "2. **Expansion**:\n",
        "   $ P(w_t | S_t^b) = \\text{GPT\\_Model}(S_t^b, w_t) $\n",
        "   $ S_{t+1}^b = S_t^b \\cup \\{w_t\\} $\n",
        "   $ P(S_{t+1}^b) = P(S_t^b) \\times P(w_t | S_t^b) $\n",
        "\n",
        "3. **Selection**:\n",
        "   $ (S_{t+1}^{(1)}, ..., S_{t+1}^{(K)}) = \\text{Top-K}(S_{t+1}^1, ..., S_{t+1}^B, K) $\n",
        "\n",
        "4. **Termination**:\n",
        "   - Terminate beams that reach the maximum length or encounter an end-of-sequence token.\n",
        "\n",
        "This process continues until all terminated or maximum-length beams are obtained.\n",
        "\n",
        "In summary, beam search efficiently explores the space of possible sequences and provides a trade-off between exploration and exploitation, helping to find high-quality sequences in sequence generation tasks such as text generation with models like GPT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def beam_search_generation(input_ids, model, beam_size) -> str:\n",
        "    outputs = input_ids\n",
        "    \n",
        "    # =========================\n",
        "    # Your code starts here (4 points)\n",
        "    # outputs should be a torch Tensor of shape [1, n]\n",
        "    # where n is the length of the generated text\n",
        "    # and the elements are the token ids of the generated text\n",
        "    # \n",
        "    # You should not use any external library like transformers's .generate().\n",
        "    # Only torch is allowed.\n",
        "    # =========================\n",
        "    \n",
        "    # =========================\n",
        "    # Your code ends here\n",
        "    # =========================\n",
        "\n",
        "    return outputs\n",
        "\n",
        "generated_ids = beam_search_generation(input_ids, model, beam_size=4)[0]\n",
        "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "print(f\"prompt: {prompt}\\ngenerated_text: {generated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task3: Sampling\n",
        "\n",
        "Sampling method randomly selects the next word based on the probability distribution of words, instead of always choosing the word with the highest probability. This increases the diversity of generated outputs but lacks stability in results.\n",
        "\n",
        "It can be represented by the formula:\n",
        "\n",
        "$ w_t \\sim \\text{Top-}k\\left(P(w_t | w_{<t})\\right) $\n",
        "\n",
        "This formula introduces a top-k constraint to the sampling process, where instead of sampling from the entire probability distribution $ P(w_t | w_{<t}) $, we first select the top $ k $ most likely words according to this distribution, and then sample from this restricted set of words. This helps in controlling the diversity of the generated outputs while still allowing for some randomness in the sampling process.\n",
        "\n",
        "In this context, $ \\text{Top-}k\\left(P(w_t | w_{<t})\\right) $ represents the selection of the top $ k $ words with the highest probabilities from the distribution $ P(w_t | w_{<t}) $, and $ w_t $ is sampled from this restricted set of words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sampling_generation(input_ids, model, topk) -> str:\n",
        "    outputs = input_ids\n",
        "    \n",
        "    # =========================\n",
        "    # Your code starts here (3 points)\n",
        "    # outputs should be a torch Tensor of shape [1, n]\n",
        "    # where n is the length of the generated text\n",
        "    # and the elements are the token ids of the generated text\n",
        "    # \n",
        "    # You should not use any external library like transformers's .generate().\n",
        "    # Only torch is allowed.\n",
        "    # =========================\n",
        "    \n",
        "    # =========================\n",
        "    # Your code ends here\n",
        "    # =========================\n",
        "\n",
        "    return outputs\n",
        "\n",
        "generated_ids = sampling_generation(input_ids, model, topk=5)[0]\n",
        "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "print(f\"prompt: {prompt}\\ngenerated_text: {generated_text}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
